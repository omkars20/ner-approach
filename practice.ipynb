{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/os/.pyenv/versions/3.12.4/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset,Features,Sequence,Value\n",
    "from transformers import AutoTokenizer,BertForTokenClassification\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with more text and labels\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is a sample text\", \n",
    "        \"Another sample text\", \n",
    "        \"A third sample text\",\n",
    "        \"Text for the fourth example\",\n",
    "        \"Fifth example text for testing\",\n",
    "        \"Sixth sample text data\",\n",
    "        \"Seventh example text\",\n",
    "        \"Eighth sample data text\",\n",
    "        \"Ninth text example for testing\",\n",
    "        \"Tenth sample text data example\"\n",
    "    ],\n",
    "    'labels': [\n",
    "        \"O O O O B-label\", \n",
    "        \"O O B-label O\", \n",
    "        \"B-label O O O\",\n",
    "        \"O O O O O O\", \n",
    "        \"O O O O O O\", \n",
    "        \"O O O O\", \n",
    "        \"O O O O\", \n",
    "        \"O O O O\", \n",
    "        \"O O O O O O\", \n",
    "        \"O O O O O O O\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a sample text</td>\n",
       "      <td>O O O O B-label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Another sample text</td>\n",
       "      <td>O O B-label O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A third sample text</td>\n",
       "      <td>B-label O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Text for the fourth example</td>\n",
       "      <td>O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifth example text for testing</td>\n",
       "      <td>O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sixth sample text data</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Seventh example text</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eighth sample data text</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ninth text example for testing</td>\n",
       "      <td>O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tenth sample text data example</td>\n",
       "      <td>O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text           labels\n",
       "0           This is a sample text  O O O O B-label\n",
       "1             Another sample text    O O B-label O\n",
       "2             A third sample text    B-label O O O\n",
       "3     Text for the fourth example      O O O O O O\n",
       "4  Fifth example text for testing      O O O O O O\n",
       "5          Sixth sample text data          O O O O\n",
       "6            Seventh example text          O O O O\n",
       "7         Eighth sample data text          O O O O\n",
       "8  Ninth text example for testing      O O O O O O\n",
       "9  Tenth sample text data example    O O O O O O O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample text', 'Another sample text', 'A third sample text', 'Text for the fourth example', 'Fifth example text for testing', 'Sixth sample text data', 'Seventh example text', 'Eighth sample data text', 'Ninth text example for testing', 'Tenth sample text data example']\n"
     ]
    }
   ],
   "source": [
    "#split text and labels into list\n",
    "texts = df['text'].to_list()\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O O O O B-label', 'O O B-label O', 'B-label O O O', 'O O O O O O', 'O O O O O O', 'O O O O', 'O O O O', 'O O O O', 'O O O O O O', 'O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "labels = df['labels'].to_list()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'a', 'sample', 'text'], ['Another', 'sample', 'text'], ['A', 'third', 'sample', 'text'], ['Text', 'for', 'the', 'fourth', 'example'], ['Fifth', 'example', 'text', 'for', 'testing'], ['Sixth', 'sample', 'text', 'data'], ['Seventh', 'example', 'text'], ['Eighth', 'sample', 'data', 'text'], ['Ninth', 'text', 'example', 'for', 'testing'], ['Tenth', 'sample', 'text', 'data', 'example']]\n"
     ]
    }
   ],
   "source": [
    "#tokenized text and split labels\n",
    "\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "print(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Labels: [['O', 'O', 'O', 'O', 'B-label'], ['O', 'O', 'B-label', 'O'], ['B-label', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_labels = [label.split() for label in labels]\n",
    "print(\"Tokenized Labels:\", tokenized_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels: ['O', 'B-label']\n"
     ]
    }
   ],
   "source": [
    "unique_labels = list(set(label for label_list in tokenized_labels for label in label_list))\n",
    "print(\"Unique Labels:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to Index: {'O': 0, 'B-label': 1}\n"
     ]
    }
   ],
   "source": [
    "# map labels to indices\n",
    "label_to_index = {\n",
    "    labels:idx \n",
    "    for idx,labels in enumerate(unique_labels)\n",
    "}\n",
    "print(\"Label to Index:\", label_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to Label: {0: 'O', 1: 'B-label'}\n"
     ]
    }
   ],
   "source": [
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "print(\"Index to Label:\", index_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Labels: [[0, 0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]]\n",
      "Length of indexed_labels: 10\n"
     ]
    }
   ],
   "source": [
    "indexed_labels = [[label_to_index[label] for label in label_list] for label_list in tokenized_labels]\n",
    "print(\"Indexed Labels:\", indexed_labels)\n",
    "print(\"Length of indexed_labels:\", len(indexed_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokenized_texts: 10\n",
      "Length of indexed_labels: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of tokenized_texts:\", len(tokenized_texts))\n",
    "print(\"Length of indexed_labels:\", len(indexed_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Texts: [['This', 'is', 'a', 'sample', 'text'], ['A', 'third', 'sample', 'text'], ['Sixth', 'sample', 'text', 'data'], ['Eighth', 'sample', 'data', 'text']]\n",
      "Valid Labels: [[0, 0, 0, 0, 1], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "valid_texts = []\n",
    "valid_labels = []\n",
    "for i in range(len(tokenized_texts)):\n",
    "    if len(tokenized_texts[i]) == len(indexed_labels[i]):\n",
    "        valid_texts.append(tokenized_texts[i])\n",
    "        valid_labels.append(indexed_labels[i])\n",
    "\n",
    "print(\"Valid Texts:\", valid_texts)\n",
    "print(\"Valid Labels:\", valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean DataFrame:\n",
      "                       text           labels\n",
      "0    This is a sample text  [0, 0, 0, 0, 1]\n",
      "1      A third sample text     [1, 0, 0, 0]\n",
      "2   Sixth sample text data     [0, 0, 0, 0]\n",
      "3  Eighth sample data text     [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "flat_texts = [\" \".join(tokens) for tokens in valid_texts]\n",
    "flat_labels = [list(map(int, labels)) for labels in valid_labels]\n",
    "\n",
    "data = {\"text\": flat_texts, \"labels\": flat_labels}\n",
    "df_clean = pd.DataFrame(data)\n",
    "\n",
    "print(\"Clean DataFrame:\\n\", df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "max_length = 128  # Set a fixed maximum length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      " Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "features = Features({\n",
    "    'text': Value('string'),\n",
    "    'labels': Sequence(Value('int64')),\n",
    "})\n",
    "\n",
    "dataset = Dataset.from_pandas(df_clean, features=features)\n",
    "\n",
    "print(\"Dataset:\\n\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 546.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Dataset:\n",
      " Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 4\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenized Dataset:\\n\", tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 729.79 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Tokenized Dataset:\n",
      " Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 4\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def align_labels_with_tokens(examples):\n",
    "    tokenized_inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        # Pad or truncate the label list to match max_length\n",
    "        if len(label) < max_length:\n",
    "            label += [-100] * (max_length - len(label))\n",
    "        elif len(label) > max_length:\n",
    "            label = label[:max_length]\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(lambda x: align_labels_with_tokens(x), batched=True)\n",
    "\n",
    "print(\"Aligned Tokenized Dataset:\\n\", tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      " Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 3\n",
      "})\n",
      "Validation Data:\n",
      " Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_test_data = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_data = train_test_data['train']\n",
    "validation_data = train_test_data['test']\n",
    "\n",
    "print(\"Training Data:\\n\", train_data)\n",
    "print(\"Validation Data:\\n\", validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-07-21 20:03:44.535782: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-21 20:03:44.544293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-21 20:03:44.552693: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-21 20:03:44.555177: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-21 20:03:44.562733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 20:03:45.713219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/os/.pyenv/versions/3.12.4/lib/python3.12/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(unique_labels))\n",
    "from transformers import Trainer, TrainingArguments, BertForTokenClassification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    ")\n",
    "\n",
    "print(\"Trainer Initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:02,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46284523606300354, 'eval_runtime': 0.1079, 'eval_samples_per_second': 9.266, 'eval_steps_per_second': 9.266, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:02<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.351688951253891, 'eval_runtime': 0.1213, 'eval_samples_per_second': 8.246, 'eval_steps_per_second': 8.246, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n",
      "INFO:__main__:Training completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3041325509548187, 'eval_runtime': 0.1362, 'eval_samples_per_second': 7.342, 'eval_steps_per_second': 7.342, 'epoch': 3.0}\n",
      "{'train_runtime': 5.524, 'train_samples_per_second': 1.629, 'train_steps_per_second': 0.543, 'train_loss': 0.5036056439081827, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "logger.info(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
