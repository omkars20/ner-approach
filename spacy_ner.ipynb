{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: ['This is a sample text', 'Another sample text', 'A third sample text', 'Text for the fourth example', 'Fifth example text for testing', 'Sixth sample text data', 'Seventh example text', 'Eighth sample data text', 'Ninth text example for testing', 'Tenth sample text data example']\n",
      "Labels: ['O O O O B-label', 'O O B-label O', 'B-label O O O', 'O O O O O O', 'O O O O O O', 'O O O O', 'O O O O', 'O O O O', 'O O O O O O', 'O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with more text and labels\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is a sample text\", \n",
    "        \"Another sample text\", \n",
    "        \"A third sample text\",\n",
    "        \"Text for the fourth example\",\n",
    "        \"Fifth example text for testing\",\n",
    "        \"Sixth sample text data\",\n",
    "        \"Seventh example text\",\n",
    "        \"Eighth sample data text\",\n",
    "        \"Ninth text example for testing\",\n",
    "        \"Tenth sample text data example\"\n",
    "    ],\n",
    "    'labels': [\n",
    "        \"O O O O B-label\", \n",
    "        \"O O B-label O\", \n",
    "        \"B-label O O O\",\n",
    "        \"O O O O O O\", \n",
    "        \"O O O O O O\", \n",
    "        \"O O O O\", \n",
    "        \"O O O O\", \n",
    "        \"O O O O\", \n",
    "        \"O O O O O O\", \n",
    "        \"O O O O O O O\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the text and labels into lists\n",
    "texts = df['text'].tolist()\n",
    "labels = df['labels'].tolist()\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: [('This is a sample text', {'entities': [(17, 21, 'B-label')]}), ('Another sample text', {'entities': [(15, 19, 'B-label')]}), ('A third sample text', {'entities': [(0, 1, 'B-label')]}), ('Text for the fourth example', {'entities': []}), ('Fifth example text for testing', {'entities': []}), ('Sixth sample text data', {'entities': []}), ('Seventh example text', {'entities': []}), ('Eighth sample data text', {'entities': []}), ('Ninth text example for testing', {'entities': []}), ('Tenth sample text data example', {'entities': []})]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for text, label in zip(texts, labels):\n",
    "    entities = []\n",
    "    tokens = text.split()\n",
    "    label_tokens = label.split()\n",
    "    start = 0\n",
    "\n",
    "    for token, lbl in zip(tokens, label_tokens):\n",
    "        if lbl != 'O':\n",
    "            entity = (start, start + len(token), lbl)\n",
    "            entities.append(entity)\n",
    "        start += len(token) + 1\n",
    "\n",
    "    train_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "print(\"Training Data:\", train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy \n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER component and optimizer ready\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.lookups import Lookups\n",
    "\n",
    "# Load pre-trained SpaCy model\n",
    "nlp =spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load lookup tables\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lexeme_norm\", {\"example\": \"example\"})\n",
    "nlp.vocab.lookups = lookups\n",
    "\n",
    "# Get the NER component and add new labels\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Disable other components in the pipeline to only train NER\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "print(\"NER component and optimizer ready\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Training loop\n",
    "n_iter = 10\n",
    "for itn in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    for batch in minibatch(train_data, size=compounding(4.0, 32.0, 1.001)):\n",
    "        for text, annotations in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.5, losses=losses)\n",
    "    print(f\"Iteration {itn}, Losses: {losses}\")\n",
    "\n",
    "print(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ner_model\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"ner_model\"\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "nlp = spacy.load(output_dir)\n",
    "\n",
    "# Test the model\n",
    "test_text = \"This is a test sentence for the NER model\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s':\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
